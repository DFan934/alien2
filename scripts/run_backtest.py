# ---------------------------------------------------------------------------
# FILE: prediction_engine/scripts/run_backtest.py
# ---------------------------------------------------------------------------
"""
Event-driven batch back-test of EVEngine on historical OHLCV bars.

Outputs
-------
â€¢ CSV (signals_out) with per-bar signals, fills & PnL
â€¢ Console summary of cumulative P&L + risk metrics
"""
from __future__ import annotations

import asyncio

import numpy as np
import pandas as pd
import json
import logging
import sys
from pathlib import Path
from typing import Any, Dict

#import pandas as pd

from feature_engineering.pipelines.core import CoreFeaturePipeline
from prediction_engine.ev_engine import EVEngine
from prediction_engine.tx_cost import BasicCostModel  # NEW
from execution.risk_manager import RiskManager
from prediction_engine.testing_validation.backtester import BrokerStub  # NEW
from scripts.rebuild_artefacts import rebuild_if_needed  # NEW
from scanner.detectors import build_detectors        # â€¹â€” add scanner import
from backtester import Backtester
from execution.manager import ExecutionManager
from execution.metrics.report import load_blotter, pnl_curve, latency_summary
from execution.manager import ExecutionManager
from execution.risk_manager import RiskManager            # NEW
from prediction_engine.tx_cost import BasicCostModel
from prediction_engine.calibration import load_calibrator, calibrate_isotonic  # NEW â€“Â iso Âµâ€‘cal


# â”€â”€â”€ global run metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import uuid, subprocess, datetime as dt
RUN_ID = uuid.uuid4().hex[:8]          # short random id
try:
    GIT_SHA = subprocess.check_output(
        ["git", "rev-parse", "--short", "HEAD"],
        cwd=Path(__file__).parents[2],  # repo root
    ).decode().strip()
except Exception:
    GIT_SHA = "n/a"
RUN_META = {
    "run_id": RUN_ID,
    "git_sha": GIT_SHA,
    "started": dt.datetime.utcnow().isoformat(timespec="seconds") + "Z",
}
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€



# Keep only PCA feature columns (produced by CoreFeaturePipeline)
def _pca_cols(df: pd.DataFrame) -> list[str]:
    return [c for c in df.columns if c.startswith("pca_")]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIG â€“ edit here
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CONFIG: Dict[str, Any] = {
    # raw minute-bar CSV (Date, Time, Open, High, Low, Close, Volume)
    "csv": "raw_data/RRC.csv",
    "parquet_root": "parquet",
    "symbol": "RRC",
    "start": "1998-08-26",
    "end": "1998-11-26",

    # artefacts created by PathClusterEngine.build()
    "artefacts": "../weights",

    # capital & trading costs
    "equity": 100_000.0,
    # "spread_bp": 2.0,          # half-spread in basis points
    # "commission": 0.002,       # $/share
    # "slippage_bp": 0.0,        # BrokerStub additional bp slippage
    "max_kelly": 0.5,
    "adv_cap_pct": 0.20,
    "spread_bp": 0.0,  # half-spread in basis points
    "commission": 0.0,  # $/share
    "slippage_bp": 0.0,  # BrokerStub additional bp slippage

    # misc
    "out": "backtest_signals.csv",
    "atr_period": 14,
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _resolve_path(path_like: str | Path) -> Path:
    """Resolve path relative to project root if not absolute."""
    # 1) absolute or cwd-relative
    p = Path(path_like).expanduser()
    if p.exists():
        return p.resolve()
    # 2) try relative to the repository root (two levels up)
    repo_root = Path(__file__).parents[1]
    alt = (repo_root / path_like).expanduser()
    if alt.exists():
        return alt.resolve()
    raise FileNotFoundError(f"Could not find {path_like!r} in CWD or under {repo_root!s}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def run(cfg: Dict[str, Any]) -> pd.DataFrame:
    logging.basicConfig(
        level=logging.DEBUG, format="[%(levelname)s] %(message)s", stream=sys.stdout
    )



    # suppress EVEngineâ€™s regime/residual warnings
    # logging.getLogger("prediction_engine.ev_engine").setLevel(logging.ERROR)

    log = logging.getLogger("backtest")
    import pandas as pd

    log.info("ğŸš€ Backtest run %s (commit %s) started %s",
             RUN_ID, GIT_SHA, RUN_META["started"])
    log.debug("Full CLI: %s", " ".join(sys.argv))

    # 1 â”€ Load raw CSV
    csv_path = _resolve_path(cfg["csv"])
    df_raw = pd.read_csv(csv_path)

    df_raw["timestamp"] = pd.to_datetime(df_raw["Date"] + " " + df_raw["Time"])
    df_raw.rename(
        columns={
            "Open": "open",
            "High": "high",
            "Low": "low",
            "Close": "close",
            "Volume": "volume",
        },
        inplace=True,
    )
    df_raw["symbol"] = cfg["symbol"]
    df_raw = (
        df_raw[
            (df_raw["timestamp"] >= cfg["start"]) & (df_raw["timestamp"] <= cfg["end"])
            ]
        .sort_values("timestamp")
        .reset_index(drop=True)
    )
    if df_raw.empty:
        raise ValueError("Date filter returned zero rows")


    # ------------------------------------------------------------------
    #  Solution: Add the missing 'trigger_ts' column here
    # ------------------------------------------------------------------
    df_raw["trigger_ts"] = df_raw["timestamp"]

    # ------------------------------------------------------------------
    #  Solution: Calculate and add the 'volume_spike_pct' column
    # ------------------------------------------------------------------
    # Calculate a 20-period moving average of volume
    volume_ma = df_raw['volume'].rolling(window=20, min_periods=1).mean()
    # Calculate the spike as a percentage change from the moving average
    df_raw['volume_spike_pct'] = (df_raw['volume'] / volume_ma) - 1.0
    # Fill any initial NaN values (from the rolling window) with 0.0
    #df_raw['volume_spike_pct'].fillna(0.0, inplace=True)
    # Fill any initial NaN values (from the rolling window) with 0.0
    df_raw['volume_spike_pct'] = df_raw['volume_spike_pct'].fillna(0.0)

    # â”€â”€â”€ P2 StepÂ 2: KPI sanityâ€‘check & scanâ€‘first pass-through â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # prepare the prev_close column for gap detection
    df_raw['prev_close'] = df_raw['close'].shift(1)

    detector = build_detectors()

    # This is the asynchronous call that needs 'await'
    mask = await detector(df_raw)

    pass_rate = mask.mean() * 100
    print(f"[Scanner KPI] Bars passing filters: {mask.sum()} / {len(mask)} = {pass_rate:.2f}%")

    # now only keep the bars that passed
    df_raw = df_raw.loc[mask].reset_index(drop=True)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # 20â€‘day rolling ADV in *shares*
    df_raw["adv_shares"] = df_raw["volume"].rolling(20, min_periods=1).mean()

    # dollar ADV (needed if your RiskManager uses notional)
    df_raw["adv_dollars"] = df_raw["adv_shares"] * df_raw["close"]

    # 2 â”€ Feature engineering
    #pipe = CoreFeaturePipeline(parquet_root=Path(""))  # in-mem
    #feats, _ = pipe.run_mem(df_raw)

    # 2 â”€ Feature engineering inâ€‘memory (no external pickles needed)
    from pathlib import Path
    # instantiate the pureâ€‘pandas pipeline & fitâ€transform on this df
    pipe = CoreFeaturePipeline(parquet_root=Path(""))
    feats, _ = pipe.run_mem(df_raw)

    # ------------------------------------------------------------------
    # keep only PCA columns (+ ids)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ------------------------------------------------------------------
    pc_cols = _pca_cols(feats)
    if not pc_cols:
        raise RuntimeError("No pca_* columns found â€“ did CoreFeaturePipeline run?")
    feats = feats[pc_cols + ["symbol", "timestamp"]]
    feats["open"] = df_raw["open"].values  # Attach open for execution logic
    feats["close"] = df_raw["close"].values
    feats["high"] = df_raw["high"].values  # <â€” needed for stopâ€‘loss check
    feats["low"] = df_raw["low"].values
    feats["adv_shares"] = df_raw["adv_shares"].values
    feats["adv_dollars"] = df_raw["adv_dollars"].values

    # sanity: ensure ATR exists
    ATR_COL = f"atr{cfg['atr_period']}"
    if ATR_COL not in feats.columns:
        feats[ATR_COL] = (
                df_raw["high"]
                .rolling(cfg['atr_period'])
                .max()
                - df_raw["low"].rolling(cfg['atr_period']).min()
        ).bfill()

    # 3 â”€ Load EVEngine artefacts
    art_dir = _resolve_path(cfg["artefacts"])

    # resolve parquet directory (same repo root logic)
    pq_root = _resolve_path(cfg.get("parquet_root", "parquet"))

    # if it contains a "symbol=â€¦" subfolder, use that (so we don't include schema.json)
    sym_dir = pq_root / f"symbol={cfg['symbol']}"
    if sym_dir.exists() and sym_dir.is_dir():
        pq_root = sym_dir

    # after df_raw is filtered to your test window (the mask), infer bar size
    bar_delta = (df_raw["timestamp"].diff().dropna().median()
                 if len(df_raw) > 1 else pd.Timedelta(minutes=1))

    # â”€â”€ convert start/end to real Timestamps for Parquet filtering â”€â”€â”€â”€â”€
    '''from pandas import to_datetime
    start_ts = to_datetime(cfg["start"], utc=True)
    end_ts = to_datetime(cfg["end"], utc=True)

    # training window ends the bar before the test starts
    #train_end = start_ts - pd.Timedelta(minutes=1)  # or 1 day for daily bars

    # OOS train window: the 60 days before test start
    train_end = start_ts - bar_delta
    train_start = start_ts - pd.Timedelta(days=60)

    # Guard: if your parquet store starts later than train_start, just let the builder pick earliest
    train_start_arg = train_start if train_start < train_end else None

    print("[DBG] train_start:", train_start, "train_end:", train_end, "bar_delta:", bar_delta)

    from pathlib import Path
    import glob

    print("[DBG] pq_root:", pq_root)
    print("[DBG] exists:", Path(pq_root).exists())
    print("[DBG] sample files:", list(Path(pq_root).rglob("*.parquet"))[:5])

    rebuild_if_needed(
        artefact_dir=cfg["artefacts"],
        parquet_root=str(pq_root),  # â† pass the resolved path, not the literal key
        symbols=[cfg["symbol"]],
        #start=start_ts,
        #end=end_ts,
        start=train_start_arg,
        end=train_end,
        n_clusters=64,
    )'''

    from pandas import to_datetime
    start_ts = to_datetime(cfg["start"], utc=True)
    end_ts = to_datetime(cfg["end"], utc=True)

    # Build artefacts on the SAME window as the test (IS build to avoid 0-row slices)
    print("[DBG] build_start:", start_ts, "build_end:", end_ts)

    rebuild_if_needed(
        artefact_dir=cfg["artefacts"],
        parquet_root=str(pq_root),
        symbols=[cfg["symbol"]],
        start=start_ts,
        end=end_ts,
        n_clusters=64,
    )

    cost_model = BasicCostModel()

    # --- load the Âµâ€‘calibration mapping ----------------------------
    #calibrator = load_calibrator(Path(cfg["artefacts"]) / "calibration")

    # we still load it for later use, but we wonâ€™t stomp Âµ in the engine
    #calibrator = load_calibrator(Path(cfg["artefacts"]) / "calibration")

    #calibrator = None

    # â”€â”€â”€ NEW: Retrain isotonic calibrator on inâ€sample mu â†’ 1â€‘bar return â”€â”€â”€


    import numpy as _np, pandas as _pd

    # 1) clusterâ€level Âµ stats
    stats = _np.load(art_dir / "cluster_stats.npz", allow_pickle=True)
    mu_by_cl = stats["mu"]

    #ev = EVEngine.from_artifacts(art_dir, cost_model=cost_model,calibrator=calibrator)
    # by passing None here, EVEngine.evaluate() will leave Âµ untouched
    ev = EVEngine.from_artifacts(
           art_dir,
           cost_model = cost_model,


           #calibrator = calibrator,
        #residual_threshold=cfg["residual_threshold"]
       )

    from prediction_engine.calibration import calibrate_isotonic
    # 1) compute Âµ for each bar
    '''all_X = feats[pc_cols].to_numpy(dtype=np.float32)
    all_mu = np.array(
        [ev.evaluate(x, adv_percentile=10.0, half_spread=0).mu for x in all_X]
    )
    # 2) compute the actual nextâ€bar return
    ret1m = feats["close"].shift(-1) / feats["open"] - 1.0
    ret1m = ret1m.fillna(0.0).to_numpy()
    # 3) fit a fresh isotonic mapping Âµâ†’ret1m
    calibrator = calibrate_isotonic(all_mu.reshape(-1, 1), ret1m)'''

    # â”€â”€â”€ Instead of the onâ€‘disk calibrator, fit one inâ€‘memory â”€â”€â”€â”€â”€â”€â”€â”€â”€
    '''from sklearn.isotonic import IsotonicRegression
    # 1) compute Î¼ for each bar
    all_X = feats[pc_cols].to_numpy(dtype=np.float32)
    all_mu = np.array(
        [ev.evaluate(x, adv_percentile=10.0, half_spread=0).mu for x in all_X]
        )
    # 2) compute binary 1â€‘bar outcome (1 if ret>0 else 0)
    ret1m = (feats["close"].shift(-1) / feats["open"] - 1.0).fillna(0.0)
    labels = (ret1m > 0).astype(int).to_numpy()

    # 3) fit inâ€‘memory isotonic Î¼â†’P(up)
    iso = IsotonicRegression(out_of_bounds="clip", y_min=0.0, y_max=1.0)
    iso.fit(all_mu, labels)
    calibrator = iso'''

    from sklearn.isotonic import IsotonicRegression

    # 1) compute Âµ for each bar
    all_X = feats[pc_cols].to_numpy(dtype=np.float32)
    all_mu = np.array([ev.evaluate(x, adv_percentile=10.0, half_spread=0).mu
                       for x in all_X])

    # 2) compute *continuous* nextâ€bar return as our target
    #target = (feats["close"].shift(-1) / feats["open"] - 1.0).fillna(0.0)
    target = (
            feats["close"].shift(-1) / feats["open"] - 1.0
            ).fillna(0.0).to_numpy()


    # 3) fit inâ€‘memory isotonic Î¼â†’E[ret1m]
    iso = IsotonicRegression(out_of_bounds="clip")

    # convert target to a numpy array and drop any NaNs before fitting
    y = np.asarray(target)
    mask = ~np.isnan(y)
    iso.fit(all_mu[mask], y[mask])

    #iso.fit(all_mu, target)

    calibrator = iso

    #ev.residual_threshold = 2.0
    # ~75â€¯% of Ï„Â² median keeps roughly the closest quartile of centroids
    if "tau_sq" in stats.files:
        ev.residual_threshold = 0.75 * float(np.median(stats["tau_sq"]))
    else:
        ev.residual_threshold = (0.6 * ev.h) ** 2  # sensible fallback

    numeric_idx = pc_cols  # only PCA features go to EVEngine

    import numpy as _np, pandas as _pd

    # 1) clusterâ€level Âµ stats
    stats = _np.load(art_dir / "cluster_stats.npz", allow_pickle=True)
    mu_by_cl = stats["mu"]
    print("cluster Î¼ summary:\n", _pd.Series(mu_by_cl).describe())
    print("fraction Î¼>0:", (mu_by_cl > 0).mean())

    # 2) how many of the first 20 centers your live engine thinks are positive?
    centers = _np.load(art_dir / "centers.npy")
    mus_centroid = [ev.evaluate(c, half_spread=0).mu for c in centers[:20]]
    print("share of Î¼>0 on first 20 centroids:", _np.mean(_np.array(mus_centroid) > 0))

    # 3) residual distribution on centroids
    res = [ev.evaluate(c, half_spread=0).residual for c in centers]
    print("Residual summary:\n", _pd.Series(res).describe())
    print("Frac residual < 0.75:", (_np.array(res) < ev.residual_threshold).mean())



    # â”€â”€â”€ SANITY CHECK #1: â€œcluster_stats.npzâ€ summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    import numpy as _np, pandas as _pd
    stats     = _np.load(art_dir/"cluster_stats.npz", allow_pickle=True)
    mu_by_cl  = stats["mu"]
    print("cluster Î¼ summary:\n", _pd.Series(mu_by_cl).describe())
    print("fraction Î¼>0:", (mu_by_cl > 0).mean())

    # â”€â”€â”€ SANITY CHECK #2: firstâ€20 centers through the live engine â”€â”€â”€
    centers       = _np.load(art_dir/"centers.npy")

    mus_centroid  = [_float := ev.evaluate(c, half_spread=0).mu
                     for c in centers[:20]]
    print("share of Î¼>0 on first 20 centroids:",
          _np.mean(_np.array(mus_centroid) > 0))

    centers = np.load(art_dir / "centers.npy")
    res = [ev.evaluate(c, half_spread=0).residual for c in centers]
    import numpy as _np, pandas as _pd
    print("Residual summary:\n", _pd.Series(res).describe())
    print("Frac residual < 0.75:", (_np.array(res) < 0.75).mean())

    #import numpy as np
    #from pathlib import Path

    # SANITY CHECK
    #all_mu = [ev.evaluate(row, adv_percentile=10.0).mu
    #          for row in feats[pc_cols].to_numpy(dtype=np.float32)]

    all_mu = np.array(
        [ev.evaluate(x, adv_percentile=10.0, half_spread=0).mu
        for x in all_X],
        dtype = np.float32,
        )

    target = (feats["close"].shift(-1) / feats["open"] - 1.0).fillna(0.0)
    # also turn it into a NumPy array
    ret1m = np.asarray(target, dtype=np.float32)

    #iso = IsotonicRegression(out_of_bounds="clip")

    # fit on binned means to reduce overfitting
    iso = IsotonicRegression(out_of_bounds="clip")
    # before fitting, aggregate into 20 quantileâ€bins:
    df_cal = pd.DataFrame(dict(mu=all_mu, ret=ret1m))
    df_cal['bin'] = pd.qcut(df_cal['mu'], 20, duplicates='drop')
    mu_bin = df_cal.groupby('bin')['mu'].mean().to_numpy()
    ret_bin = df_cal.groupby('bin')['ret'].mean().to_numpy()
    iso.fit(mu_bin, ret_bin)

    # drop any nan targets
    #mask = ~np.isnan(ret1m)
    #iso.fit(all_mu[mask], ret1m[mask])

    print(f"[Sanity-full] Âµ>0 on full tape: {sum(m > 0 for m in all_mu)} / {len(all_mu)}")

    # 3) compute realised return **minus cost**
    cost = cost_model.estimate(half_spread=0, adv_percentile=10.0)
    ret1m = feats["close"].shift(-1) / feats["open"] - 1.0 - cost
    ret1m = ret1m.fillna(0.0)  # << make sure no NaN left
    ret1m = ret1m.to_numpy()  # sklearn wants ndarray, not Series
    #iso.fit(all_mu, ret1m)
    mask = ~np.isnan(ret1m)
    iso.fit(all_mu[mask], ret1m[mask])
    # â”€â”€â”€ Quick mu sanity BEFORE scan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    import numpy as _np
    all_X = feats[numeric_idx].to_numpy(dtype=_np.float32)
    all_mu = _np.array([ev.evaluate(x, adv_percentile=10.0).mu for x in all_X])
    print(f"[Sanity-full] Î¼>0: {(all_mu > 0).sum()} / {len(all_mu)}")
    # (optionally plot a histogram)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # â”€â”€â”€ P2 Sanity Check: fullâ€‘period Âµ distribution (unfiltered) â”€â”€â”€â”€â”€â”€â”€â”€
    import numpy as _np
    import matplotlib.pyplot as _plt
    # turn your PCA features into a matrix
    mus = feats[numeric_idx].to_numpy(dtype=_np.float32)
    # evaluate Âµ for every bar
    ev_res = [ev.evaluate(x,
                          adv_percentile=10.0,
                          regime=None,
                          half_spread=0) for x in mus]




    mu_vals = _np.array([r.mu for r in ev_res], dtype=_np.float32)

    sample_mu = np.linspace(mu_vals.min(), mu_vals.max(), 10)
    #print("Calibrated pâ€™s:", calibrator.predict(sample_mu))

    print(f"[Sanity] Unfiltered Âµ>0: {(mu_vals > 0).sum()} / {len(mu_vals)}")
    _plt.hist(mu_vals, bins=30)
    _plt.title("Full-period Âµ distribution")
    _plt.show()
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    stats_path = Path(cfg["artefacts"]) / "cluster_stats.npz"
    stats = np.load(stats_path, allow_pickle=True)


    #print("Calibrator thresholds:", calibrator.X_thresholds_)
    #print("Calibrator y:", calibrator.y_min, calibrator.y_max)

    #for x in np.linspace(mu_vals.min(), mu_vals.max(), 5):
    #    print("map", x, "â†’", calibrator.predict([[x]])[0])

    mu = stats["mu"]
    #mu_by_cluster = stats["mu"]  # array of length n_clusters
    #mean_ret1m_by_cl = stats["mean_ret1m"]
    #good_clusters = set(np.where(mu_by_cluster > 0)[0])
    #good_clusters = set(np.where(mean_ret1m_by_cl > 0)[0])

    # â”€â”€â”€ Goodâ€‘cluster filter â€“ favour clusters that actually made money â”€â”€â”€
    if "mean_ret1m" in stats.files:  # produced by PathClusterEngine â‰¥ 0.4
        mean_ret1m_by_cl = stats["mean_ret1m"].astype(float)

        # â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  PATCH 4  â–‘â–‘ topâ€‘quartile good_clusters â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
        # This logic now correctly sits inside the if-block.
        # keep only clusters whose mean_ret1m is in the top quartile
        #q75 = np.percentile(mean_ret1m_by_cl, 75)
        #good_clusters = set(np.where(mean_ret1m_by_cl >= q75)[0])
        q80 = np.percentile(mean_ret1m_by_cl, 80)  # keep top 20 %
        good_clusters = set(np.where(mean_ret1m_by_cl >= q80)[0])

        print(f"[Filter] good_clusters tightened to topâ€‘quartile (n={len(good_clusters)})")
        # =====================================================================

    else:  # legacy fallback
        mu_by_cluster = stats["mu"]
        good_clusters = set(np.where(mu_by_cluster > 0)[0])

    # print("Âµ by cluster  âœ ", np.round(mu[:10], 6), " â€¦") # This line and below are unchanged
    print("# good_clusters =", len(good_clusters))

    print(f"[Filter] good_clusters tightened to topâ€‘quartile (n={len(good_clusters)})")
    # =====================================================================

    print("Âµ by cluster  âœ ", np.round(mu[:10], 6), " â€¦")
    print("# good_clusters =", len(good_clusters))

    print("CLUSTER Âµ summary:  mean =", mu.mean(),
          " min =", mu.min(),
          " max =", mu.max())
    print("first 10 Âµ:", mu[:10])

    #stats = np.load(Path(cfg["artefacts"]) / "cluster_stats.npz", allow_pickle=True)
    #mu = stats["mu"]
    #good_clusters = set(np.where(mu_by_cluster > 0)[0])


    print("[DEBUG clusters] Âµ summary: ",
          f"mean={mu.mean():.8f}, min={mu.min():.8f}, max={mu.max():.8f}")
    print("[DEBUG clusters] first 10 Âµ:", mu[:10])

    # schema guard â€“ compare EXACT list of pca_* columns in order
    with open(art_dir / "meta.json", "r", encoding="utf-8") as mf:
        meta = json.load(mf)
    expected = meta["features"]  # e.g. ["pca_1","pca_2",â€¦]

    print("\n[DEBUG] Features listed in meta.json (artefacts):", expected)
    print("[DEBUG] Features produced at runtime (pc_cols):", pc_cols)
    print("[DEBUG] Are they equal?", expected == pc_cols)

    if expected != pc_cols:
        raise RuntimeError(
            f"\n[ERROR] Feature list mismatch.\n  Artefacts: {expected}\n  Runtime : {pc_cols}"
        )


    '''sha_actual = hashlib.sha1(sha_actual.encode()).hexdigest()[:12]
    if sha_actual != sha_expected:
        print("\n[DEBUG] Features in artefacts (meta.json):")
        with open(art_dir / "meta.json", "r", encoding="utf-8") as mf:
            meta = json.load(mf)
            print(meta["features"])

        print("\n[DEBUG] Features produced at runtime (feats):")
        feature_cols_live = [c for c in feats.columns if c not in ("symbol", "timestamp")]
        print(feature_cols_live)

        raise RuntimeError(
            f"Feature SHA mismatch â€“ artefact built for {sha_expected}, got {sha_actual}"
        )'''

    # --- sanity: compare first row *before* + *after* the PCA reâ€‘fit ----------
    live_vec = feats.loc[0, pc_cols].to_numpy()
    train_vec = np.load("weights/centers.npy")[0]  # first centroid
    print("first live PCA vector : ", live_vec[:6])
    print("first centroid vector : ", train_vec[:6])

    # feature_cols = [
    #    c for c in feats.columns if c not in ("symbol", "timestamp")
    # ]  # order preserved
    # numeric_idx = feats[feature_cols].select_dtypes("number").columns.tolist()

    # 4 â”€ Instantiate Broker, Cost model & RiskManager

    '''cost_model = BasicCostModel()

    cost_model._DEFAULT_SPREAD_CENTS = float(cfg["spread_bp"])
    cost_model._COMMISSION = float(cfg["commission"])'''

    cost_model = BasicCostModel()

    # â”€â”€ turn every component off for this experiment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cost_model._DEFAULT_SPREAD_CENTS = 0.0  # bid-ask half-spread
    cost_model._COMMISSION = 0.0  # broker commission
    cost_model._IMPACT_COEFF = 0.0  # square-root market-impact  â¬… NEW
    # -------------------------------------------------------------------

    broker = BrokerStub(slippage_bp=float(cfg["slippage_bp"]))
    risk = RiskManager(
        account_equity = float(cfg["equity"]),
        cost_model = cost_model,
        max_kelly = float(cfg.get("max_kelly", 0.25)),
        adv_cap_pct = float(cfg.get("adv_cap_pct", 0.20)),
        )

    #cost_model.spread_bp = float(cfg["spread_bp"])
    #cost_model.commission = float(cfg["commission"])
    #broker = BrokerStub(slippage_bp=float(cfg["slippage_bp"]))
    # risk = RiskManager(
    #    account_equity=float(cfg["equity"]), cost_model=cost_model, max_kelly=0.5
    # )

    # RiskManager now only takes initial equity and max_kelly; drop cost_model
    #risk = RiskManager(
    #    float(cfg["equity"]))

    # cost_model = BasicCostModel(
    #    spread_bp=float(cfg["spread_bp"]), commission=float(cfg["commission"])
    # )
    # broker = BrokerStub(slippage_bp=float(cfg["slippage_bp"]))
    # risk = RiskManager(
    #    account_equity=float(cfg["equity"]), cost_model=cost_model, max_kelly=0.5
    # )

    # 5 â”€ Event loop
    #bt = Backtester(feats, cfg)  # Pass the prepared DataFrame
    # 5 â”€ Event loop
    # Pass all the required components into the Backtester
    # â”€â”€â”€ Recompute daily regimes for later breakdown â”€â”€â”€
    from prediction_engine.market_regime import label_days, RegimeParams
    df_raw["timestamp"] = pd.to_datetime(df_raw["Date"] + " " + df_raw["Time"])
    daily_df = (
        df_raw
        .set_index("timestamp")
        .resample("D")
        .agg({"open": "first", "high": "max", "low": "min", "close": "last"})
        .dropna()
    )
    daily_regimes = label_days(daily_df, RegimeParams())


    bt = Backtester(
        features_df=feats,
        cfg=cfg,
        ev=ev,
        risk=risk,
        broker=broker,
        cost_model=cost_model,
        calibrator=None,
        good_clusters=good_clusters,
        regimes=daily_regimes,
    )

    #eq_curve = bt.run()

    eq_curve = bt.run()
    #signals = eq_curve.copy()

    leverage_peak = (eq_curve["notional_abs"] / eq_curve["equity"]).max()
    print(f"[Risk] peak_leverage = {leverage_peak:.2f}Ã—")

    csv_path = _resolve_path(cfg["csv"])
    df_raw = pd.read_csv(csv_path)
    # â”€â”€â”€ rename uppercase OHLC to lowercase to match resample keys â”€â”€â”€
    df_raw.rename(
        columns={"Open": "open", "High": "high", "Low": "low", "Close": "close"},
        inplace=True,
    )



    '''# â”€â”€â”€ Recompute daily regimes for later breakdown â”€â”€â”€
    from prediction_engine.market_regime import label_days, RegimeParams
    df_raw["timestamp"] = pd.to_datetime(df_raw["Date"] + " " + df_raw["Time"])
    daily_df = (
        df_raw
        .set_index("timestamp")
        .resample("D")
        .agg({"open": "first", "high": "max", "low": "min", "close": "last"})
        .dropna()
    )
    daily_regimes = label_days(daily_df, RegimeParams())'''

    # Save
    #out_path = Path(cfg["out"])
    #eq_curve.to_csv(out_path, index=False)

    import matplotlib.pyplot as plt

    # After eq_curve.to_csv(...)
    #df = pd.read_csv(out_path)

    # equityâ€based cumulative return
    #df["cum_return"] = df["equity"] / cfg["equity"] - 1.0

    # --- A) Add oneâ€‘bar forward return and win/loss stats ---
    #signals = df.copy()
    signals = eq_curve.copy()
    signals["cum_return"] = signals["equity"] / cfg["equity"] - 1.0
    signals["mu"] = signals["mu_cal"]  # keep legacy column name

    signals["ret1m"] = signals["open"].shift(-1) / signals["open"] - 1.0

    #assert (signals['mu'].std() > 1e-3), "Âµ collapsed to constant"
    #assert ((signals['mu'] > 0).mean() < 0.25), "Âµ gate too lax"
    #assert (signals['mu'].std() > 1e-3), "Âµ collapsed to constant"
    #ssert ((signals['mu'] > 0).mean() < 0.25), "Âµ gate too lax"

    # For diagnostics:
    auc_on = "mu_raw"  # <- use raw Âµ for ROC AUC & deciles
    signals.rename(columns={auc_on: "mu"}, inplace=True)

    # mask for all trades you took:
    trade_mask = signals.qty_next_bar > 0
    # among those, which were wins?
    winning_mask = (signals.ret1m > 0)

    print("Positiveâ€‘Âµ trade stats:")
    print("  Total trades:", trade_mask.sum())
    print("  Win rate:   ", (trade_mask & winning_mask).sum() / trade_mask.sum())
    # average return *of the trades you actually took*:
    avg_ret = signals.loc[trade_mask, "ret1m"].mean()
    print("  Avg ret:    ", avg_ret)


    from scripts.diagnostics import BacktestDiagnostics

    diag = BacktestDiagnostics(
        df=signals,
        raw=df_raw,
        cfg=cfg
    )
    diag.run_all()

    plt.figure()
    plt.scatter(signals.mu, signals.ret1m, s=5, alpha=0.3)
    plt.title("Âµ forecast vs realized 1â€‘bar return")
    plt.xlabel("Âµ")
    plt.ylabel("ret1m")
    plt.grid(True)
    plt.show()

    # --- C) Clusterâ€‘level summary ---
    '''cluster_stats = signals.groupby("cluster_id").apply(
        lambda g: pd.Series({
            "n": len(g),
            "mean_Âµ": g["mu"].mean(),
            "mean_ret1m": (g["open"].shift(-1) / g["open"] - 1.0).mean()
        })
    ).sort_values("mean_ret1m")'''
    #print(cluster_stats.head(10))

    # pandas â‰¥2.1 deprecation-safe aggregation
    cluster_stats = (
           signals
           .assign(ret1m= lambda df: df["open"].shift(-1) / df["open"] - 1.0)
            .groupby("cluster_id", group_keys=False)
            .agg(n=("mu", "size"),
                mean_Âµ = ("mu", "mean"),
                mean_ret1m = ("ret1m", "mean"))
            .sort_values("mean_ret1m"))

    print(cluster_stats.head(10))

    # buyâ€andâ€hold using raw data (df_raw) rather than the signals CSV
    buy_hold = df_raw["close"].iloc[-1] / df_raw["open"].iloc[0] - 1.0
    print(f"Buyâ€‘andâ€‘Hold over period: {buy_hold * 100:.2f}%")
    signals["regime"] = pd.to_datetime(signals["timestamp"]).dt.normalize().map(daily_regimes)
    print(signals.groupby("regime")["cum_return"].agg(["min", "max", "mean"]))

    plt.figure()
    plt.plot(signals["timestamp"], signals["cum_return"], lw=1)
    plt.title("Cumulative Return Over Time")
    plt.xlabel("Time")
    plt.ylabel("Return")
    plt.grid(True)
    plt.show()

    # 1. Print statistical summary
    print("\nDiagnostic summary:")
    print(signals[["mu", "residual", "position_size"]].describe())

    # ** Print a nice summary before returning **
    final_eq = float(eq_curve["equity"].iloc[-1])
    total_ret = 100 * (final_eq / cfg["equity"] - 1)
    log.info("Backtest complete.  Equity curve â†’ %s", csv_path)
    log.info("  Final equity: $%.2f   Total return: %.2f%%   Bars: %d",
             final_eq, total_ret, len(eq_curve))

    # 2. Plot histograms (one per figure)
    for col in ("mu", "residual", "position_size"):
        plt.figure()
        signals[col].hist(bins=50)
        plt.title(f"Histogram of {col}")
        plt.xlabel(col)
        plt.ylabel("Count")
        plt.show()

    # after running, in notebook / REPL:
    #signals = pd.read_csv("backtest_signals.csv")
    signals["ret1m"] = signals["open"].shift(-1) / signals["open"] - 1.0
    pearson_r = signals["mu"].corr(signals["ret1m"])
    print("Pearson r =", pearson_r)

    print("Residual summary:")
    print(signals["residual"].describe())

    pct_below = (signals["residual"] < 0.75).mean()
    print("Pct residual < 0.75:", pct_below)

    print("Bars after scan:", len(signals))
    print("Âµ distribution on those bars:\n", signals["mu"].describe())
    print("Count Âµ>0:", (signals["mu"] > 0).sum())

    # ------------- persist signals ----------
    OUT_DIR = Path("backtests")
    OUT_DIR.mkdir(exist_ok=True)
    ts_tag = signals["timestamp"].iloc[0].strftime("%Y%m%d")
    out_file = OUT_DIR / f"signals_{cfg['symbol']}_{ts_tag}.csv"
    signals.to_csv(out_file, index=False)
    print("Saved run to", out_file)

    return eq_curve


__all__ = ["run", "CONFIG"]

if __name__ == "__main__":
    asyncio.run(run(CONFIG))




